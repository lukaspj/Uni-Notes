\input{preamble}

\begin{document}
    \section{Subjects}
    \begin{itemize}
        \item VC Dimension
        \item Bias Variance
        \item Regularization
        \item Validation
    \end{itemize}
    \section{Notes}
    
    \subsection{Preceding discussion}
    \textit{Chapter 2 in Learning from data.}\\
    What we want to minimize, is the out-of-sample error:
    \begin{equation*}
    E_{out}(g) = \Ex_{x,y}\left(e\left(g(x), y\right)\right)
    \end{equation*}
    We can't minimize this, however what we can minimize is the in-sample-error:
    \begin{equation*}
    E_{in}(g)=\frac{1}{\|D\|} \sum_{(x,y) \in D} e\left(g(x), y\right)
    \end{equation*}
    So we are hoping, or aiming for, that the generalization error 
    $E_{in}-E_{out}$ should be small, so when we minimize $E_{in}$ it benefits 
    $E_{out}$.\\
    Recall that the Hoeffding inequality provides a way to bound the 
    generalization error:
    \begin{equation*}
    \Pr[|E_{in}(g) - E_{out}(g)| > \epsilon] \leq 2Me^{-2\epsilon^2N}
    \end{equation*}
    We can rephrase this, by introducing a tolerance level $\delta$ and assert 
    with probability at least $1 - \delta$ that: 
    \begin{equation*}
    E_{out}(g) \leq E_{in}(g) + \sqrt{\frac{1}{2N}\ln\frac{2M}{\delta}}
    \end{equation*}
    We notice here, that the bound depends on $M$, which is the size of the 
    hypothesis set. Unfortunately, most problems has infinite hypotheses, and 
    thus the bound will become meaningless as it goes towards infinity. So we 
    want to replace $M$ by something that stays meaningful as $M$ goes to 
    infinity. 
    
    To this end we introduce the growth function, which will formalize the 
    effective number of hypotheses. Furthermore, a \textit{dichotomy} is an 
    $N$-tuple, generated by a hypothesis, which splits the data into two 
    groups.
    
    The set of dichotomies generated by the hypothesis set $\Hy$ on the points 
    $x_1,\dots,x_n$ is defined by:
    \begin{equation}
    \Hy(x_1,\dots,x_N)= \{(h(x_1),\dots,h(x_N)) \,|\, h \in \Hy)\}
    \end{equation}
    One can think of the dichotomies as being the hypothesis set as seen 
    through the eyes of just $N$ points. We can then define the growth function 
    as:
    \begin{equation}
    m_\Hy(N)=\max\limits_{x_1, \dots, x_n \in X} |\Hy(x_1,\dots, x_N)|
    \end{equation}
    I.e. the maximum number of dichotomies that can be generated by $\Hy$ on 
    any $N$ points. If we just look at binary classification, then the upper 
    limit on the amount of dichotomies for a data-set of size $N$ is: 
    \begin{equation}
    m_\Hy(N) \leq 2^N
    \end{equation}
    If $\Hy$ is capable of generating all possible dichotomies on the data-set, 
    then $m_\Hy(n) = 2^N$ and we say that $\Hy$ shatter $x_1,\dots,x_n$. If 
    there is no such data-set of size $k$ that can be shattered by $\Hy$ then 
    we say that $k$ is a breakpoint for $\Hy$.
    
    If there is such a breakpoint $k$, then we know that $m_\Hy(k) < 2^N$. We 
    define $B(N,k)$ as the maximum number of dichotomies on $N$ points, such 
    that no subset of size $k$ can be shattered by these dichotomies. We can 
    then see that:
    
    \begin{equation*}
    m\Hy(N) \leq B(N,k)\, \text{ if $k$ is a break point for }\Hy
    \end{equation*}
    
    Sauer's lemma then states that:
    \begin{equation*}
    B(N,k) \leq \sum_{i=0}^{k-1}\begin{pmatrix}
    N\\
    i
    \end{pmatrix}
    \end{equation*}
    
    Which means that:
    \begin{equation*}
        m_\Hy(N) \leq \sum_{i=0}^{k-1} \begin{pmatrix}
        N\\i
        \end{pmatrix}
    \end{equation*}
    
    We then see that, if $\Hy$ has a breakpoint then $m_\Hy(N)$ has a 
    polynomial bound. This is important because when $m_\Hy(N)$ has a 
    polynomial bound, the generalization error will go to zero as $N 
    \rightarrow \infty$
    
    \subsection{VC Dimension}
    The Vapnik-Chervonenkis dimension of a hypothesis set $\Hy$, denoted by 
    $d_{vc}(\Hy)$ or simply $d_{vc}$ is the largest value of $N$ for which 
    $m_\Hy(N)=2^N$. If $m_\Hy(N)=2^N$ for all $N$, then $d_{vc}(\Hy)=\infty$.
    
    It follows then, that if $d_{vc}$ is the VC dimension for $\Hy$, then $k = 
    d_{vc} + 1$ is a breakpoint and there are no smaller breakpoints. We can 
    therefore rewrite the previous sum in terms of the VC dimension:
    
    \begin{equation*}
        m_\Hy(N) \leq \sum_{i=0}^{d_{vc}} \begin{pmatrix}
        N\\i
        \end{pmatrix}
    \end{equation*}
    
    We then arrive at the VC Generalization Bound:
    \begin{equation}
        E_{out}(g) \leq E_{in}(g) + \sqrt{\frac{8}{N} \ln 
        \frac{4m_\Hy(2N)}{\delta}}
    \end{equation}
    with probability $\geq 1-\delta$
    
    \subsection{Bias-Variance decomposition}
    \textit{Page 62}\\
    The out-of-sample error for hypothesis $g^{(D)}$ learned on data $D$ is:
    \begin{equation*}
        E_{out}(g^{(D)}) = \Ex_x\left[(g^{(D)}(x)-f(x))^2\right]
    \end{equation*}
    Where $\Ex_x$ denotes the expected value with respect to $x$ (based on the 
    probability distribution on the input space $X$)
    
    We can generalize this, to remove the dependence on a specific data-set by 
    taking the expectation with respect to all data-sets:
    \begin{align*}
        \Ex_D\left[E_{out}(g^{(D)})\right] &=
        \Ex_D\left[\Ex_x\left[(g^{(D)}(x)-f(x))^2\right]\right]\\
            &= \Ex_x\left[\Ex_D\left[(g^{(D)}(x)-f(x))^2\right]\right]\\
            &= \Ex_x\left[\Ex_D\left[g^{(D)}(x)^2\right] - 
            2\Ex_D\left[g^{(D)}(x)\right]f(x)+f(x)^2)\right]
    \end{align*}
    
    The term $\Ex_D\left[g^{(D)}(x)\right]$ gives an 'average function', which 
    we denote by $\bar{g}(x)$ it can be seen as an average function as a result 
    of many data-sets $D_1,\dots,D_K$ where:
    \begin{equation}
        \bar{g}(x)\simeq \frac{1}{K} \sum_{k=1}^{K}g_k(x)
    \end{equation}
    
    We can now rewrite the expected out-of-sample error in terms of $\bar{g}$:
    \begin{align*}
        &\Ex_D\left[E_{out}(g^{(D)})\right]\\
        &=\Ex_x\left[\Ex_D\left[g^{(D)}(x)^2\right] - 2\bar{g}(x)f(x)+ 
        f(x)^2\right]\\
        &=\Ex_x\left[\Ex_D\left[g^{(D)}(x)^2\right]-\bar{g}(x)^2 + \bar{g}(x)^2 
        - 2\bar{g}(x)f(x)+f(x)^2\right]\\
        &=\Ex_x\left[\Ex_D\left[(g^{(D)}(x) - \bar{g}(x))^2\right] + 
        (\bar{g}(x)-f(x))^2\right]
    \end{align*}
    
    The term to the right $(\bar{g}(x)-f(x))^2$ measure how much the average 
    function we would learn using the $D$ different data-sets deviates from the 
    target function, we call this term the bias:
    \begin{equation*}
        \text{bias}(x)=(\bar{g}(x)-f(x))^2
    \end{equation*}
    The other term, $\Ex_D\left[(g^{(D)}(x)-\bar{g}(x))^2\right]$ is what we 
    call the variance, which measures the variation in the final hypothesis:
    \begin{equation}
        \text{var}(x)=\Ex_D\left[(g^{(D)}(x)-\bar{g}(x))^2\right]
    \end{equation}
    We thus arrive at the bias-variance decomposition of out-of-sample error:
    \begin{align*}
        \Ex_D\left[E_{out}(g^{(D)})\right] &= \Ex_x\left[\text{bias}(x) + 
        \text{var}(x)\right]\\
            &= \text{bias} + \text{var}\\
        \text{bias} &= \Ex_x\left[\text{bias(x)}\right]\\
        \text{var} &= \Ex_x\left[\text{var(x)}\right]
    \end{align*}
    \textit{Bias:} How well can we actually fit - on average\\
    \textit{Variance:} How much will data samples lead me astray - on average
    VC-Dimension captures expressiveness/capacity of hypothesis spaces and 
    relate them to generalization. Leads to out-of-sample error equals 
    in-sample-error + model complexity:
    \begin{equation*}
        E_{out}(h) \leq E_{in}(h) + \Omega(N,\Hy, \delta)
    \end{equation*}
    \begin{equation*}
        \Omega(N, \Hy, \delta) = \sqrt{\frac{8}{N}\ln\frac{4m_\Hy(2N)}{\delta}}
    \end{equation*}
    
    We cannot compute actual bias and variance in practice, since they depend 
    on the target function and input probability distribution. So it is a 
    conceptual tool, which is helpful when it comes to developing a model.
    
    There are two typical goals: we want to reduce the variance without 
    significantly increasing bias et vice versa. These goals are achieved 
    through heuristics, regularization being one them.
\end{document}